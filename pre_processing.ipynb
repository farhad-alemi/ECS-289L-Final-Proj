{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECS 289L Term Project - Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Downloading the Raw Vulnerability Dataset (DiverseVul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./.conda/lib/python3.11/site-packages (5.2.0)\n",
      "Requirement already satisfied: beautifulsoup4 in ./.conda/lib/python3.11/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in ./.conda/lib/python3.11/site-packages (from gdown) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in ./.conda/lib/python3.11/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.11/site-packages (from gdown) (4.66.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.conda/lib/python3.11/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests[socks]->gdown) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in ./.conda/lib/python3.11/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#installing the required package(s)\n",
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset_name = 'DiverseVul_EntireDataset.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTANT ###\n",
    "# Since it is practically unfeasible to include the entire DiverseVul raw dataset (736.9 MB) as part of the submission, we commented the following lines of code.\n",
    "# Interested readers are welcome to uncomment the code, and download the entire raw dataset.\n",
    "# Nevertheless, next steps work regardless of whether the raw dataset is re-downloaded from scratch.\n",
    "\n",
    "\n",
    "# import os\n",
    "\n",
    "# # checks if raw dataset is already there\n",
    "# if os.path.exists(f'./{raw_dataset_name}'):\n",
    "#     print('DiverseVul dataset already exists.')\n",
    "# else:\n",
    "#     !gdown --id 12IWKhmLhq7qn5B_iXgn5YerOQtkH-6RG -O $raw_dataset_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filtering Dataset for Sidechannel-Related Entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCA-filtered dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_and_write_to_file(input_file, output_file, condition_func):\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                line = line.strip()\n",
    "                json_object = json.loads(line)\n",
    "                # Check if the JSON object meets the specified condition\n",
    "                if condition_func(json_object):\n",
    "                    # Write the JSON object to the output file\n",
    "                    outfile.write(json.dumps(json_object) + \"\\n\")\n",
    "            except:\n",
    "                print(f\"Error reading line '{line}'.\")\n",
    "\n",
    "\n",
    "# DiverseVul Condition\n",
    "def condition_DiverseVul(json_object):\n",
    "    return (\n",
    "        (\"side-channel\" in json_object.get(\"message\").lower())\n",
    "        or (\"sidechannel\" in json_object.get(\"message\").lower())\n",
    "        or (\"side channel\" in json_object.get(\"message\").lower())\n",
    "    )\n",
    "\n",
    "\n",
    "input_file_path = f\"./{raw_dataset_name}\"\n",
    "sca_filtered_dataset_name = \"SCA-filtered_Dataset.json\"\n",
    "output_file_path = f\"./{sca_filtered_dataset_name}\"\n",
    "\n",
    "# checks if the filtered dataset is already there\n",
    "if os.path.exists(f'./{sca_filtered_dataset_name}'):\n",
    "    print('SCA-filtered dataset already exists.')\n",
    "else:\n",
    "    parse_and_write_to_file(input_file_path, output_file_path, condition_DiverseVul)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Converting the Filtered Dataset into OpenAI Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted_SCA-filtered dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "# Function to read JSON objects from a file\n",
    "def read_json_objects_from_file(file_path):\n",
    "    json_arr = []\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        # Read the entire file as a single string\n",
    "        data = file.read()\n",
    "\n",
    "        # Split the string into individual JSON objects\n",
    "        json_strings = data.split(\"}\\n{\")\n",
    "\n",
    "        # Process each JSON string\n",
    "        for i, json_string in enumerate(json_strings):\n",
    "            if i == 0:\n",
    "                json_string += \"}\"  # Add back the closing bracket for the first object\n",
    "            elif i == len(json_strings) - 1:\n",
    "                json_string = (\"{\" + json_string)  # Add back the opening bracket for the last object\n",
    "            else:\n",
    "                json_string = (\"{\" + json_string + \"}\")  # Add both opening and closing brackets for other objects\n",
    "            json_arr.append(json.loads(json_string))\n",
    "\n",
    "    return json_arr\n",
    "\n",
    "\n",
    "def filter_msg(text):\n",
    "    pattern = r\"([^\\s]+)-by:\\s+([^\\s]+)([^<]+)<[^>]+>\"\n",
    "\n",
    "    return re.sub(pattern, \"\", text, count=0)\n",
    "\n",
    "\n",
    "def transform_json_objects(input_file, output_file):\n",
    "    with open(output_file, \"w\") as o_json:\n",
    "        in_data = read_json_objects_from_file(input_file)\n",
    "\n",
    "        for in_obj in in_data:\n",
    "            filtered_obj = {\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You're a smart assistant, aiding a cybersecurity researcher in pinpointing sidechannel-related CWEs.\",\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": \"Would you please review the following C code, list any CWE(s) found (if any), and provide a descriptive explanation: \",  # append code-snippet\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": \"The code contains the following CWE(s): \",  # append CWE(s) & message\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            filtered_obj[\"messages\"][1][\"content\"] += f\"\\\"{in_obj['func']}\\\"?\"\n",
    "            filtered_obj[\"messages\"][2][\"content\"] += f\"{', '.join(in_obj['cwe'])}. {filter_msg(in_obj['message'])}?\"\n",
    "\n",
    "            o_json.write(json.dumps(filtered_obj) + \"\\n\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "input_file_path = f\"./{sca_filtered_dataset_name}\"  # input JSON file\n",
    "preprocessing_product_name = \"Formatted_SCA-filtered_Dataset.jsonl\" # output JSONL file\n",
    "output_file_path = f\"./{preprocessing_product_name}\"\n",
    "\n",
    "\n",
    "# checks if the formatted&filtered dataset is already there\n",
    "if os.path.exists(output_file_path):\n",
    "    print('Formatted_SCA-filtered dataset already exists.')\n",
    "else:\n",
    "    transform_json_objects(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test splits already exist.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def train_test_split(input_file, train_file, test_file, split_ratio=0.8, seed=None):\n",
    "    with open(input_file, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "    \n",
    "    # Shuffle entries\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    random.shuffle(entries)\n",
    "    \n",
    "    # Split entries\n",
    "    split_idx = int(len(entries) * split_ratio)\n",
    "    train_entries = entries[:split_idx]\n",
    "    test_entries = entries[split_idx:]\n",
    "    \n",
    "    # Write to train file\n",
    "    with open(train_file, 'w') as f:\n",
    "        f.writelines(train_entries)\n",
    "    \n",
    "    # Write to test file\n",
    "    with open(test_file, 'w') as f:\n",
    "        f.writelines(test_entries)\n",
    "\n",
    "# Example usage\n",
    "input_file_path = f\"{preprocessing_product_name}\"\n",
    "train_file = \"train.jsonl\"\n",
    "val_file = \"val.jsonl\"\n",
    "\n",
    "# checks if the train/test files are already there\n",
    "if os.path.exists(f'./{train_file}') and os.path.exists(f'./{val_file}'):\n",
    "    print('Train/Test splits already exist.')\n",
    "else:\n",
    "    train_test_split(input_file_path, f\"./{train_file}\", f\"./{val_file}\", split_ratio=0.8)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
